{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 8586207,
     "sourceType": "datasetVersion",
     "datasetId": 5135502
    },
    {
     "sourceId": 181535459,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, BatchNormalization, LayerNormalization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\n\nimport re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-06-05T15:01:42.552169Z",
     "iopub.execute_input": "2024-06-05T15:01:42.552688Z",
     "iopub.status.idle": "2024-06-05T15:02:01.051256Z",
     "shell.execute_reply.started": "2024-06-05T15:01:42.552644Z",
     "shell.execute_reply": "2024-06-05T15:02:01.049505Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "2024-06-05 15:01:45.906371: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-05 15:01:45.906571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-05 15:01:46.090470: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# 1. Read the data and inspect first 10 rows\nfile_path = \"/kaggle/input/phrase-data/phrases_data.txt\"\ndata = pd.read_csv(file_path, sep='\\t', names=['phrases'])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T15:02:04.890650Z",
     "iopub.execute_input": "2024-06-05T15:02:04.891417Z",
     "iopub.status.idle": "2024-06-05T15:02:04.932478Z",
     "shell.execute_reply.started": "2024-06-05T15:02:04.891349Z",
     "shell.execute_reply": "2024-06-05T15:02:04.930961Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 2. Clean training data\ndef clean_data(data):\n    # Remove punctuation\n    data['cleaned'] = data['phrases'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n    # Convert to lowercase\n    data['cleaned'] = data['cleaned'].str.lower()\n\n    # Remove whitespace\n    data['cleaned'] = data['cleaned'].str.strip()\n\n    # # Remove stop words\n    # stop_words = set(stopwords.words('english'))\n    # data['cleaned'] = data['cleaned'].apply(lambda x:\n    #                                         ' '.join([word for word in x.split()\n    #                                         if word not in stop_words]))\n\n    # Lemmatization\n#     lemmatizer = WordNetLemmatizer()\n#     data['cleaned'] = data['cleaned'].apply(lambda x:\n#                                             ' '.join([lemmatizer.lemmatize(word)\n#                                             for word in x.split()]))\nclean_data(data)\ndata.head(10)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T15:02:09.981736Z",
     "iopub.execute_input": "2024-06-05T15:02:09.982166Z",
     "iopub.status.idle": "2024-06-05T15:02:10.063293Z",
     "shell.execute_reply.started": "2024-06-05T15:02:09.982131Z",
     "shell.execute_reply": "2024-06-05T15:02:10.062082Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            phrases  \\\n0                              Let's try something.   \n1                            I have to go to sleep.   \n2  Today is June 18th and it is Muiriel's birthday!   \n3                                Muiriel is 20 now.   \n4                        The password is \"Muiriel\".   \n5                              I will be back soon.   \n6                          I'm at a loss for words.   \n7                       This is never going to end.   \n8                    I just don't know what to say.   \n9                           That was an evil bunny.   \n\n                                          cleaned  \n0                              lets try something  \n1                           i have to go to sleep  \n2  today is june 18th and it is muiriels birthday  \n3                               muiriel is 20 now  \n4                         the password is muiriel  \n5                             i will be back soon  \n6                          im at a loss for words  \n7                      this is never going to end  \n8                    i just dont know what to say  \n9                          that was an evil bunny  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrases</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Let's try something.</td>\n      <td>lets try something</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I have to go to sleep.</td>\n      <td>i have to go to sleep</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Today is June 18th and it is Muiriel's birthday!</td>\n      <td>today is june 18th and it is muiriels birthday</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Muiriel is 20 now.</td>\n      <td>muiriel is 20 now</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The password is \"Muiriel\".</td>\n      <td>the password is muiriel</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I will be back soon.</td>\n      <td>i will be back soon</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I'm at a loss for words.</td>\n      <td>im at a loss for words</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>This is never going to end.</td>\n      <td>this is never going to end</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>I just don't know what to say.</td>\n      <td>i just dont know what to say</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>That was an evil bunny.</td>\n      <td>that was an evil bunny</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# vocabulary = list(set(' '.join(data['cleaned']).replace('\\n','').split(' ')))\n# vocab_dictionary = {}\n# for strings, texts in enumerate(vocabulary):\n#     vocab_dictionary[texts] = strings\n# total_words = len(vocab_dictionary) + 1\n# print(total_words)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\n# tokenizer.fit_on_texts(data['cleaned'])\n# total_words = len(tokenizer.word_index) + 1\n\n# print(f\"Total number of words: {total_words}\")\n# print(f\"All words: {tokenizer.word_index.keys()}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def create_input_output_pairs(sentences):\n    max_sequence_length = 0\n    inputs, outputs = [], []\n    for sentence in sentences:\n        all_words = sentence.split()\n        if len(all_words) <= 3:\n            inputs.append(' '.join(all_words[:-1]))\n            outputs.append(all_words[-1])\n            continue\n        if len(all_words) > max_sequence_length:\n            max_sequence_length = len(all_words)\n        for i in range(3, len(all_words)):\n            input_seq = ' '.join(all_words[:i])\n            output_word = all_words[i]\n            inputs.append(input_seq)\n            outputs.append(output_word)\n    return inputs, outputs, max_sequence_length\n\ninputs, outputs, max_sequence_length = create_input_output_pairs(data['cleaned'])\nprint(inputs[:10])\nprint(outputs[:10])\n\nprint(len(inputs))\nprint(len(outputs))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T15:02:18.206065Z",
     "iopub.execute_input": "2024-06-05T15:02:18.206487Z",
     "iopub.status.idle": "2024-06-05T15:02:18.291673Z",
     "shell.execute_reply.started": "2024-06-05T15:02:18.206453Z",
     "shell.execute_reply": "2024-06-05T15:02:18.290296Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "['lets try', 'i have to', 'i have to go', 'i have to go to', 'today is june', 'today is june 18th', 'today is june 18th and', 'today is june 18th and it', 'today is june 18th and it is', 'today is june 18th and it is muiriels']\n['something', 'go', 'to', 'sleep', '18th', 'and', 'it', 'is', 'muiriels', 'birthday']\n51500\n51500\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Get the embedding of inputs\nimport tensorflow_hub as hub\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"  \nget_embedding = hub.load(module_url)\n\ninput_embeddings = np.array(get_embedding(inputs))\noutput_embeddings = np.array(get_embedding(outputs))\nprint(input_embeddings.shape)\nprint(output_embeddings.shape)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T15:03:52.600856Z",
     "iopub.execute_input": "2024-06-05T15:03:52.601267Z",
     "iopub.status.idle": "2024-06-05T15:04:05.479989Z",
     "shell.execute_reply.started": "2024-06-05T15:03:52.601234Z",
     "shell.execute_reply": "2024-06-05T15:04:05.478667Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "(51500, 512)\n(51500, 512)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\ntokenizer.fit_on_texts(outputs)\ntotal_words = len(tokenizer.word_index) + 1\n\nprint(f\"Total number of words: {total_words}\")\n\n# tokenized_labels = [tokenizer.word_index[each_word] for each_word in outputs]\n# print(tokenized_labels[:2])\n# print(tokenized_labels)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T01:08:59.003920Z",
     "iopub.execute_input": "2024-06-05T01:08:59.004305Z",
     "iopub.status.idle": "2024-06-05T01:08:59.444126Z",
     "shell.execute_reply.started": "2024-06-05T01:08:59.004275Z",
     "shell.execute_reply": "2024-06-05T01:08:59.443070Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of words: 6112\n[135, 36]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "one_hot_labels = tf.keras.utils.to_categorical(tokenized_labels, num_classes=total_words)\nprint(one_hot_labels.shape)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T01:12:15.288508Z",
     "iopub.execute_input": "2024-06-05T01:12:15.288872Z",
     "iopub.status.idle": "2024-06-05T01:12:15.437583Z",
     "shell.execute_reply.started": "2024-06-05T01:12:15.288844Z",
     "shell.execute_reply": "2024-06-05T01:12:15.436533Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "(51500, 6112)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# pad sequences\n# input_sequences = np.array(pad_sequences(tokenized_phrases,\n#                                          maxlen=max_sequence_length - 1,\n#                                          padding='pre'))\n# print(input_sequences[0])\n# print(len(input_sequences))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_train_validation_test_data(input_sequences, one_hot_labels):\n    # Split dataset into training, validation, and test sets\n    # First, split the whole dataset into 80% training and validation + 20% testing\n    X_train_val, X_test, y_train_val, y_test = train_test_split(\n    input_sequences, one_hot_labels, test_size=0.20, random_state=42)\n    \n    # Next, split the 80% training and validation set into 50% validation + 50% testing\n    X_train, X_val, y_train, y_val = train_test_split(\n    X_train_val, y_train_val, test_size=0.5, random_state=42)\n    \n    train_data, test_data = train_test_split(\n        input_sequences, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\nX_train, y_train, X_val, y_val, X_test, y_test = get_train_validation_test_data(input_embeddings, one_hot_labels)\n# print(X_train[0])\nprint(y_train[0])\nprint(X_train.shape)\nprint(y_train.shape)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T01:18:50.454247Z",
     "iopub.execute_input": "2024-06-05T01:18:50.455428Z",
     "iopub.status.idle": "2024-06-05T01:18:52.047084Z",
     "shell.execute_reply.started": "2024-06-05T01:18:50.455357Z",
     "shell.execute_reply": "2024-06-05T01:18:52.045966Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": "[0. 0. 0. ... 0. 0. 0.]\n(20600, 512)\n(20600, 6112)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "def create_model(total_words, hidden_size, optimizer):\n    model = Sequential()\n#     model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n#     model.add(LSTM(hidden_size, input_shape=[512]))  # , dropout=0.2, recurrent_dropout=0.2\n    model.add(Dense(hidden_size, input_shape=[512], activation='relu'))\n    model.add(Dense(total_words, activation='softmax')) \n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n    return model\n\n# Function to decay the learning rate\ndef scheduler(epoch, lr):\n    min_lr = 0.001  # Set the minimum learning rate\n    if epoch < 10:\n        return lr\n    else:\n        new_lr = lr * np.exp(-0.1)  # Decays the learning rate by 1% every epoch after the 10th\n#         return new_lr\n        return max(new_lr, min_lr)\n\n# Set the initial learning rate\ninitial_learning_rate = 0.01\n\n# Compile the model with an optimizer\noptimizer = Adam(learning_rate=initial_learning_rate)\n\nlr_scheduler = LearningRateScheduler(scheduler)\n\n# Include the epoch in the file name (uses `str.format`)\ncheckpoint_path = \"/kaggle/working/model_lstm.keras\"\ncallback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    save_best_only=True,\n    monitor='val_loss',\n    verbose=1)\n\nmodel = create_model(total_words, hidden_size=128, optimizer=optimizer)\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=64,\n                    validation_data=(X_val, y_val), \n                    callbacks=[lr_scheduler, callback], verbose=1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T01:35:25.332954Z",
     "iopub.execute_input": "2024-06-05T01:35:25.333378Z",
     "iopub.status.idle": "2024-06-05T01:40:22.357381Z",
     "shell.execute_reply.started": "2024-06-05T01:35:25.333349Z",
     "shell.execute_reply": "2024-06-05T01:40:22.356128Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/50\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - acc: 0.0613 - loss: 7.1281\nEpoch 1: val_loss improved from inf to 6.61400, saving model to /kaggle/working/model_lstm.keras\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 25ms/step - acc: 0.0614 - loss: 7.1272 - val_acc: 0.0697 - val_loss: 6.6140 - learning_rate: 0.0100\nEpoch 2/50\n\u001B[1m319/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.0766 - loss: 6.0105\nEpoch 2: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - acc: 0.0767 - loss: 6.0097 - val_acc: 0.0765 - val_loss: 6.6631 - learning_rate: 0.0100\nEpoch 3/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.0914 - loss: 5.3729\nEpoch 3: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - acc: 0.0914 - loss: 5.3727 - val_acc: 0.0800 - val_loss: 6.9645 - learning_rate: 0.0100\nEpoch 4/50\n\u001B[1m320/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.1094 - loss: 4.7217\nEpoch 4: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - acc: 0.1094 - loss: 4.7215 - val_acc: 0.0809 - val_loss: 7.4873 - learning_rate: 0.0100\nEpoch 5/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.1605 - loss: 3.9439\nEpoch 5: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - acc: 0.1604 - loss: 3.9444 - val_acc: 0.0775 - val_loss: 8.4195 - learning_rate: 0.0100\nEpoch 6/50\n\u001B[1m319/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - acc: 0.2508 - loss: 3.2898\nEpoch 6: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - acc: 0.2506 - loss: 3.2910 - val_acc: 0.0661 - val_loss: 9.2948 - learning_rate: 0.0100\nEpoch 7/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.3513 - loss: 2.7817\nEpoch 7: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 20ms/step - acc: 0.3511 - loss: 2.7824 - val_acc: 0.0646 - val_loss: 9.8629 - learning_rate: 0.0100\nEpoch 8/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.4174 - loss: 2.4133\nEpoch 8: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 21ms/step - acc: 0.4172 - loss: 2.4142 - val_acc: 0.0638 - val_loss: 10.6280 - learning_rate: 0.0100\nEpoch 9/50\n\u001B[1m319/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - acc: 0.4730 - loss: 2.1251\nEpoch 9: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 19ms/step - acc: 0.4726 - loss: 2.1269 - val_acc: 0.0661 - val_loss: 11.2667 - learning_rate: 0.0100\nEpoch 10/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - acc: 0.5149 - loss: 1.9179\nEpoch 10: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - acc: 0.5147 - loss: 1.9188 - val_acc: 0.0637 - val_loss: 11.8421 - learning_rate: 0.0100\nEpoch 11/50\n\u001B[1m320/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.5526 - loss: 1.7320\nEpoch 11: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - acc: 0.5524 - loss: 1.7329 - val_acc: 0.0660 - val_loss: 12.3988 - learning_rate: 0.0090\nEpoch 12/50\n\u001B[1m319/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - acc: 0.5953 - loss: 1.5426\nEpoch 12: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - acc: 0.5950 - loss: 1.5439 - val_acc: 0.0622 - val_loss: 12.8089 - learning_rate: 0.0082\nEpoch 13/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.6270 - loss: 1.4039\nEpoch 13: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - acc: 0.6269 - loss: 1.4045 - val_acc: 0.0643 - val_loss: 13.2189 - learning_rate: 0.0074\nEpoch 14/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - acc: 0.6638 - loss: 1.2849\nEpoch 14: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - acc: 0.6636 - loss: 1.2854 - val_acc: 0.0654 - val_loss: 13.5992 - learning_rate: 0.0067\nEpoch 15/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - acc: 0.6853 - loss: 1.1895\nEpoch 15: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 20ms/step - acc: 0.6851 - loss: 1.1900 - val_acc: 0.0639 - val_loss: 14.0437 - learning_rate: 0.0061\nEpoch 16/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7026 - loss: 1.1226\nEpoch 16: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - acc: 0.7024 - loss: 1.1230 - val_acc: 0.0657 - val_loss: 14.2785 - learning_rate: 0.0055\nEpoch 17/50\n\u001B[1m318/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7190 - loss: 1.0547\nEpoch 17: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.7187 - loss: 1.0556 - val_acc: 0.0667 - val_loss: 14.6076 - learning_rate: 0.0050\nEpoch 18/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7329 - loss: 0.9898\nEpoch 18: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.7328 - loss: 0.9901 - val_acc: 0.0676 - val_loss: 14.9399 - learning_rate: 0.0045\nEpoch 19/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7462 - loss: 0.9483\nEpoch 19: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - acc: 0.7461 - loss: 0.9486 - val_acc: 0.0666 - val_loss: 15.1625 - learning_rate: 0.0041\nEpoch 20/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7619 - loss: 0.9014\nEpoch 20: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 17ms/step - acc: 0.7618 - loss: 0.9017 - val_acc: 0.0674 - val_loss: 15.4350 - learning_rate: 0.0037\nEpoch 21/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7696 - loss: 0.8604\nEpoch 21: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - acc: 0.7695 - loss: 0.8607 - val_acc: 0.0655 - val_loss: 15.5837 - learning_rate: 0.0033\nEpoch 22/50\n\u001B[1m320/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7784 - loss: 0.8280\nEpoch 22: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - acc: 0.7783 - loss: 0.8284 - val_acc: 0.0656 - val_loss: 15.7893 - learning_rate: 0.0030\nEpoch 23/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7898 - loss: 0.7962\nEpoch 23: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - acc: 0.7897 - loss: 0.7965 - val_acc: 0.0670 - val_loss: 16.0520 - learning_rate: 0.0027\nEpoch 24/50\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.7932 - loss: 0.7836\nEpoch 24: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.7932 - loss: 0.7836 - val_acc: 0.0652 - val_loss: 16.1905 - learning_rate: 0.0025\nEpoch 25/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8017 - loss: 0.7465\nEpoch 25: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - acc: 0.8017 - loss: 0.7467 - val_acc: 0.0662 - val_loss: 16.3691 - learning_rate: 0.0022\nEpoch 26/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8045 - loss: 0.7384\nEpoch 26: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 18ms/step - acc: 0.8045 - loss: 0.7385 - val_acc: 0.0682 - val_loss: 16.5946 - learning_rate: 0.0020\nEpoch 27/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8176 - loss: 0.6961\nEpoch 27: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8175 - loss: 0.6964 - val_acc: 0.0677 - val_loss: 16.7290 - learning_rate: 0.0018\nEpoch 28/50\n\u001B[1m319/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8195 - loss: 0.6918\nEpoch 28: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8194 - loss: 0.6921 - val_acc: 0.0677 - val_loss: 16.7861 - learning_rate: 0.0017\nEpoch 29/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8252 - loss: 0.6718\nEpoch 29: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8252 - loss: 0.6720 - val_acc: 0.0674 - val_loss: 16.9575 - learning_rate: 0.0015\nEpoch 30/50\n\u001B[1m318/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8355 - loss: 0.6488\nEpoch 30: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8352 - loss: 0.6493 - val_acc: 0.0679 - val_loss: 17.0814 - learning_rate: 0.0014\nEpoch 31/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8282 - loss: 0.6539\nEpoch 31: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8282 - loss: 0.6540 - val_acc: 0.0675 - val_loss: 17.1666 - learning_rate: 0.0012\nEpoch 32/50\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - acc: 0.8355 - loss: 0.6422\nEpoch 32: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 17ms/step - acc: 0.8354 - loss: 0.6422 - val_acc: 0.0680 - val_loss: 17.2391 - learning_rate: 0.0011\nEpoch 33/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8348 - loss: 0.6348\nEpoch 33: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8348 - loss: 0.6349 - val_acc: 0.0687 - val_loss: 17.3478 - learning_rate: 0.0010\nEpoch 34/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8404 - loss: 0.6246\nEpoch 34: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - acc: 0.8403 - loss: 0.6246 - val_acc: 0.0679 - val_loss: 17.4301 - learning_rate: 0.0010\nEpoch 35/50\n\u001B[1m318/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8424 - loss: 0.6149\nEpoch 35: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8423 - loss: 0.6151 - val_acc: 0.0675 - val_loss: 17.5186 - learning_rate: 0.0010\nEpoch 36/50\n\u001B[1m320/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8410 - loss: 0.6157\nEpoch 36: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8409 - loss: 0.6158 - val_acc: 0.0683 - val_loss: 17.6137 - learning_rate: 0.0010\nEpoch 37/50\n\u001B[1m318/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8478 - loss: 0.5986\nEpoch 37: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8476 - loss: 0.5989 - val_acc: 0.0675 - val_loss: 17.6499 - learning_rate: 0.0010\nEpoch 38/50\n\u001B[1m319/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - acc: 0.8466 - loss: 0.5954\nEpoch 38: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 17ms/step - acc: 0.8465 - loss: 0.5957 - val_acc: 0.0670 - val_loss: 17.7564 - learning_rate: 0.0010\nEpoch 39/50\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8422 - loss: 0.5998\nEpoch 39: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8422 - loss: 0.5998 - val_acc: 0.0685 - val_loss: 17.8517 - learning_rate: 0.0010\nEpoch 40/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8497 - loss: 0.5858\nEpoch 40: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8497 - loss: 0.5859 - val_acc: 0.0671 - val_loss: 17.8930 - learning_rate: 0.0010\nEpoch 41/50\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8510 - loss: 0.5771\nEpoch 41: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8510 - loss: 0.5771 - val_acc: 0.0675 - val_loss: 17.9999 - learning_rate: 0.0010\nEpoch 42/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8548 - loss: 0.5675\nEpoch 42: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8547 - loss: 0.5677 - val_acc: 0.0682 - val_loss: 18.0881 - learning_rate: 0.0010\nEpoch 43/50\n\u001B[1m318/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8595 - loss: 0.5548\nEpoch 43: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8593 - loss: 0.5552 - val_acc: 0.0678 - val_loss: 18.1463 - learning_rate: 0.0010\nEpoch 44/50\n\u001B[1m319/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - acc: 0.8611 - loss: 0.5445\nEpoch 44: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 18ms/step - acc: 0.8610 - loss: 0.5449 - val_acc: 0.0675 - val_loss: 18.2590 - learning_rate: 0.0010\nEpoch 45/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8572 - loss: 0.5538\nEpoch 45: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8572 - loss: 0.5539 - val_acc: 0.0673 - val_loss: 18.3362 - learning_rate: 0.0010\nEpoch 46/50\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8587 - loss: 0.5432\nEpoch 46: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8587 - loss: 0.5432 - val_acc: 0.0675 - val_loss: 18.3949 - learning_rate: 0.0010\nEpoch 47/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8559 - loss: 0.5463\nEpoch 47: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8559 - loss: 0.5464 - val_acc: 0.0680 - val_loss: 18.4828 - learning_rate: 0.0010\nEpoch 48/50\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8617 - loss: 0.5355\nEpoch 48: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8617 - loss: 0.5356 - val_acc: 0.0672 - val_loss: 18.5768 - learning_rate: 0.0010\nEpoch 49/50\n\u001B[1m321/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - acc: 0.8633 - loss: 0.5323\nEpoch 49: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8633 - loss: 0.5324 - val_acc: 0.0679 - val_loss: 18.6826 - learning_rate: 0.0010\nEpoch 50/50\n\u001B[1m319/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - acc: 0.8628 - loss: 0.5259\nEpoch 50: val_loss did not improve from 6.61400\n\u001B[1m322/322\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - acc: 0.8627 - loss: 0.5261 - val_acc: 0.0669 - val_loss: 18.7126 - learning_rate: 0.0010\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "test_loss, test_mse = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"Test Loss: {test_loss}, Test MSE: {test_mse}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
