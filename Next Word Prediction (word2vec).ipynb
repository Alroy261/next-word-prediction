{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 8586207,
     "sourceType": "datasetVersion",
     "datasetId": 5135502
    },
    {
     "sourceId": 8614988,
     "sourceType": "datasetVersion",
     "datasetId": 5156155
    },
    {
     "sourceId": 53922,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 45215
    }
   ],
   "dockerImageVersionId": 30732,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, BatchNormalization, LayerNormalization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\n\nimport re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-06-05T16:02:53.023554Z",
     "iopub.execute_input": "2024-06-05T16:02:53.024057Z",
     "iopub.status.idle": "2024-06-05T16:03:00.615157Z",
     "shell.execute_reply.started": "2024-06-05T16:02:53.024001Z",
     "shell.execute_reply": "2024-06-05T16:03:00.613890Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "2024-06-05 16:02:54.114761: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-05 16:02:54.114848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-05 16:02:54.118489: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# 1. Read the data and inspect first 10 rows\nfile_path = \"/kaggle/input/phrase-data/phrases_data.txt\"\ndata = pd.read_csv(file_path, sep='\\t', names=['phrases'])\n\n# 2. Clean training data\ndef clean_data(data):\n    # Remove punctuation\n    data['cleaned'] = data['phrases'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n    # Convert to lowercase\n    data['cleaned'] = data['cleaned'].str.lower()\n\n    # Remove whitespace\n    data['cleaned'] = data['cleaned'].str.strip()\n\n    # # Remove stop words\n    # stop_words = set(stopwords.words('english'))\n    # data['cleaned'] = data['cleaned'].apply(lambda x:\n    #                                         ' '.join([word for word in x.split()\n    #                                         if word not in stop_words]))\n\n    # Lemmatization\n#     lemmatizer = WordNetLemmatizer()\n#     data['cleaned'] = data['cleaned'].apply(lambda x:\n#                                             ' '.join([lemmatizer.lemmatize(word)\n#                                             for word in x.split()]))\nclean_data(data)\ndata.head(10)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T16:03:03.949427Z",
     "iopub.execute_input": "2024-06-05T16:03:03.950920Z",
     "iopub.status.idle": "2024-06-05T16:03:04.063955Z",
     "shell.execute_reply.started": "2024-06-05T16:03:03.950874Z",
     "shell.execute_reply": "2024-06-05T16:03:04.062661Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "execution_count": 2,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            phrases  \\\n0                              Let's try something.   \n1                            I have to go to sleep.   \n2  Today is June 18th and it is Muiriel's birthday!   \n3                                Muiriel is 20 now.   \n4                        The password is \"Muiriel\".   \n5                              I will be back soon.   \n6                          I'm at a loss for words.   \n7                       This is never going to end.   \n8                    I just don't know what to say.   \n9                           That was an evil bunny.   \n\n                                          cleaned  \n0                              lets try something  \n1                           i have to go to sleep  \n2  today is june 18th and it is muiriels birthday  \n3                               muiriel is 20 now  \n4                         the password is muiriel  \n5                             i will be back soon  \n6                          im at a loss for words  \n7                      this is never going to end  \n8                    i just dont know what to say  \n9                          that was an evil bunny  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrases</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Let's try something.</td>\n      <td>lets try something</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I have to go to sleep.</td>\n      <td>i have to go to sleep</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Today is June 18th and it is Muiriel's birthday!</td>\n      <td>today is june 18th and it is muiriels birthday</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Muiriel is 20 now.</td>\n      <td>muiriel is 20 now</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The password is \"Muiriel\".</td>\n      <td>the password is muiriel</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I will be back soon.</td>\n      <td>i will be back soon</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I'm at a loss for words.</td>\n      <td>im at a loss for words</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>This is never going to end.</td>\n      <td>this is never going to end</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>I just don't know what to say.</td>\n      <td>i just dont know what to say</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>That was an evil bunny.</td>\n      <td>that was an evil bunny</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import gensim\nword2vec_model_path = '/kaggle/input/word2vec-model/GoogleNews-vectors-negative300.bin'\nmodel = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "similar_words = model.most_similar('sleep', topn=5)\nprint(similar_words)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec\n\n# Tokenizing the corpus\ntokenized_corpus = [word_tokenize(each_sentence) for each_sentence in data['cleaned']]\nprint(tokenized_corpus[:4])\n\n# Training the Word2Vec model\ncustomized_model = Word2Vec(sentences=tokenized_corpus, vector_size=200, window=10, min_count=1, workers=4)\n\n# Saving the model\ncustomized_model.save(\"/kaggle/working/word2vec.model\")\n\nsimilar_words = customized_model.wv.most_similar('butterfly', topn=5)\nprint(similar_words)\nvocabulary = customized_model.wv\nprint(len(vocabulary))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T16:03:21.902843Z",
     "iopub.execute_input": "2024-06-05T16:03:21.903269Z",
     "iopub.status.idle": "2024-06-05T16:03:56.506025Z",
     "shell.execute_reply.started": "2024-06-05T16:03:21.903235Z",
     "shell.execute_reply": "2024-06-05T16:03:56.504116Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "[['lets', 'try', 'something'], ['i', 'have', 'to', 'go', 'to', 'sleep'], ['today', 'is', 'june', '18th', 'and', 'it', 'is', 'muiriels', 'birthday'], ['muiriel', 'is', '20', 'now']]\n[('ended', 0.9704582095146179), ('wash', 0.9702301025390625), ('island', 0.9702083468437195), ('looks', 0.9701722264289856), ('schools', 0.969989538192749)]\n7247\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "def create_input_output_pairs(sentences):\n    max_sequence_length = 0\n    inputs, outputs = [], []\n    for sentence in sentences:\n        all_words = sentence.split()\n        if len(all_words) <= 3:\n            inputs.append(' '.join(all_words[:-1]))\n            outputs.append(all_words[-1])\n            continue\n        if len(all_words) > max_sequence_length:\n            max_sequence_length = len(all_words)\n        for i in range(3, len(all_words)):\n            input_seq = ' '.join(all_words[:i])\n            output_word = all_words[i]\n            inputs.append(input_seq)\n            outputs.append(output_word)\n    return inputs, outputs, max_sequence_length\n\ninputs, outputs, max_sequence_length = create_input_output_pairs(data['cleaned'])\nprint(inputs[:10])\nprint(outputs[:10])\n\nprint(len(inputs))\nprint(len(outputs))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-05T16:05:16.898271Z",
     "iopub.execute_input": "2024-06-05T16:05:16.899850Z",
     "iopub.status.idle": "2024-06-05T16:05:16.986744Z",
     "shell.execute_reply.started": "2024-06-05T16:05:16.899799Z",
     "shell.execute_reply": "2024-06-05T16:05:16.985397Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "['lets try', 'i have to', 'i have to go', 'i have to go to', 'today is june', 'today is june 18th', 'today is june 18th and', 'today is june 18th and it', 'today is june 18th and it is', 'today is june 18th and it is muiriels']\n['something', 'go', 'to', 'sleep', '18th', 'and', 'it', 'is', 'muiriels', 'birthday']\n51500\n51500\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "def sentence_embedding(sentence, model):\n    # Tokenize the sentence into words\n    words = sentence.split()\n    \n    # Get the vector for each word if it exists in the model's vocabulary\n    word_vectors = [customized_model.wv[word] for word in words if word in customized_model.wv]\n    \n    # Handle cases where the sentence may not contain any words with vectors (rare)\n    if len(word_vectors) == 0:\n        # Return a zero vector if none of the words were in the model's vocabulary\n        return np.zeros(model.vector_size)\n    else:\n        # Compute the mean of these vectors\n        return np.mean(word_vectors, axis=0)\n\n# Assuming 'model' is your pre-trained Word2Vec model\ninput_embeddings = [sentence_embedding(sentence, model) for sentence in inputs]\noutput_embeddings = [sentence_embedding(word, model) for word in outputs]",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_train_validation_test_data(input_sequences, one_hot_labels):\n    # Split dataset into training, validation, and test sets\n    # First, split the whole dataset into 80% training and validation + 20% testing\n    X_train_val, X_test, y_train_val, y_test = train_test_split(\n    input_sequences, one_hot_labels, test_size=0.20, random_state=42)\n    \n    # Next, split the 80% training and validation set into 50% validation + 50% testing\n    X_train, X_val, y_train, y_val = train_test_split(\n    X_train_val, y_train_val, test_size=0.5, random_state=42)\n    \n    train_data, test_data = train_test_split(\n        input_sequences, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\nX_train, y_train, X_val, y_val, X_test, y_test = get_train_validation_test_data(input_embeddings, output_embeddings)\n# print(X_train[0])\n# print(y_train[0])\nprint(X_train.shape)\nprint(y_train.shape)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
